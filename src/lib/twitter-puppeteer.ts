import puppeteer from 'puppeteer'
import { writeFileSync, existsSync, mkdirSync } from 'fs'
import { join } from 'path'
import sharp from 'sharp'

interface TwitterPost {
  text: string
  imageUrl?: string
  date: string
  menuName: string
  ingredients?: string[]
}

export class TwitterPuppeteerScraper {
  private browser: any = null
  private page: any = null

  async initialize() {
    this.browser = await puppeteer.launch({
      headless: true,
      args: [
        '--no-sandbox',
        '--disable-setuid-sandbox',
        '--disable-dev-shm-usage',
        '--disable-gpu',
        '--no-first-run',
        '--no-zygote',
        '--single-process'
      ]
    })
    
    this.page = await this.browser.newPage()
    
    // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®š
    await this.page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
    
    // ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆã‚’è¨­å®š
    await this.page.setViewport({ width: 1200, height: 800 })
  }

  async scrapeUserPosts(username: string, maxPosts: number = 20): Promise<TwitterPost[]> {
    if (!this.page) {
      throw new Error('Browser not initialized')
    }

    const posts: TwitterPost[] = []
    
    try {
      console.log(`Navigating to @${username} profile...`)
      
      // Twitterã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
      await this.page.goto(`https://twitter.com/${username}`, {
        waitUntil: 'networkidle2',
        timeout: 30000
      })
      
      // ãƒšãƒ¼ã‚¸ãŒèª­ã¿è¾¼ã¾ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
      await new Promise(resolve => setTimeout(resolve, 3000))
      
      // ãƒ‡ãƒãƒƒã‚°ç”¨ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ
      await this.page.screenshot({ path: 'twitter-debug.png', fullPage: true })
      console.log('Screenshot saved as twitter-debug.png')
      
      // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ãªãŒã‚‰ãƒ„ã‚¤ãƒ¼ãƒˆã‚’åé›†
      let previousHeight = 0
      let scrollAttempts = 0
      const maxScrollAttempts = 10
      
      while (posts.length < maxPosts && scrollAttempts < maxScrollAttempts) {
        // ç¾åœ¨ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å–å¾—
        const tweets = await this.page.evaluate(() => {
          // ã‚ˆã‚ŠæŸ”è»Ÿãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ä½¿ç”¨
          const articles = document.querySelectorAll('article, [data-testid="tweet"], [role="article"]')
          const results: any[] = []
          
          articles.forEach((article: any) => {
            try {
              // ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™ï¼‰
              let text = ''
              const textSelectors = [
                '[data-testid="tweetText"]',
                '[lang]',
                '.css-901oao',
                'span[style*="color"]'
              ]
              
              for (const selector of textSelectors) {
                const textElement = article.querySelector(selector)
                if (textElement) {
                  text = textElement.innerText || textElement.textContent || ''
                  if (text.length > 10) break // æ„å‘³ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã£ãŸã‚‰åœæ­¢
                }
              }
              
              if (!text || text.length < 5) return
              
              // å®šé£Ÿã‚’å«ã‚€ãƒ„ã‚¤ãƒ¼ãƒˆã®ã¿
              if (!text.includes('å®šé£Ÿ')) return
              
              // æ™‚é–“ã‚’å–å¾—
              const timeElement = article.querySelector('time')
              const dateTime = timeElement ? timeElement.getAttribute('datetime') : new Date().toISOString()
              
              // ç”»åƒã‚’å–å¾—ï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™ï¼‰
              let imageUrl = ''
              const imageSelectors = [
                '[data-testid="tweetPhoto"] img',
                'img[alt*="Image"]',
                'img[src*="pbs.twimg.com"]'
              ]
              
              for (const selector of imageSelectors) {
                const imageElement = article.querySelector(selector)
                if (imageElement && imageElement.src) {
                  imageUrl = imageElement.src
                  break
                }
              }
              
              results.push({
                text: text.trim(),
                imageUrl,
                date: dateTime
              })
            } catch (e) {
              console.log('Error processing tweet element:', e)
            }
          })
          
          return results
        })
        
        // æ–°ã—ã„ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å‡¦ç†
        for (const tweet of tweets) {
          if (posts.length >= maxPosts) break
          
          const menuName = this.extractMenuName(tweet.text)
          
          // é‡è¤‡ãƒã‚§ãƒƒã‚¯
          if (!posts.some(p => p.text === tweet.text)) {
            posts.push({
              text: tweet.text,
              imageUrl: tweet.imageUrl,
              date: tweet.date,
              menuName,
              ingredients: await this.inferIngredients(menuName)
            })
            
            console.log(`Found: ${menuName}`)
          }
        }
        
        // ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
        const currentHeight = await this.page.evaluate(() => document.body.scrollHeight)
        
        if (currentHeight === previousHeight) {
          scrollAttempts++
        } else {
          scrollAttempts = 0
        }
        
        await this.page.evaluate(() => {
          window.scrollTo(0, document.body.scrollHeight)
        })
        
        await new Promise(resolve => setTimeout(resolve, 2000))
        previousHeight = currentHeight
      }
      
      console.log(`Scraped ${posts.length} menu posts`)
      
    } catch (error) {
      console.error('Scraping error:', error)
    }
    
    return posts
  }

  private extractMenuName(tweetText: string): string {
    // ã€‡ã€‡å®šé£Ÿã‹ã‚‰çŒ®ç«‹åã‚’æŠ½å‡º
    const match = tweetText.match(/(.+?)å®šé£Ÿ/)
    if (match) {
      return match[1].trim()
    }
    
    // ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    const firstLine = tweetText.split('\n')[0]
    return firstLine.replace('å®šé£Ÿ', '').trim()
  }

  private async inferIngredients(menuName: string): Promise<string[]> {
    const ingredientMap: { [key: string]: string[] } = {
      'å”æšã’': ['é¶è‚‰', 'é†¤æ²¹', 'ç”Ÿå§œ', 'ã«ã‚“ã«ã', 'ç‰‡æ —ç²‰', 'æ²¹'],
      'ç”Ÿå§œç„¼ã': ['è±šè‚‰', 'ç‰ã­ã', 'ç”Ÿå§œ', 'é†¤æ²¹', 'ã¿ã‚Šã‚“', 'é…’'],
      'ãƒãƒ³ãƒãƒ¼ã‚°': ['ç‰›ã²ãè‚‰', 'è±šã²ãè‚‰', 'ç‰ã­ã', 'ãƒ‘ãƒ³ç²‰', 'åµ', 'ç‰›ä¹³'],
      'é®­': ['é®­', 'å¡©', 'èƒ¡æ¤’'],
      'ã•ã°': ['ã•ã°', 'å‘³å™Œ', 'ã¿ã‚Šã‚“', 'é…’'],
      'ã‚«ãƒ¬ãƒ¼': ['ç‰›è‚‰', 'ã˜ã‚ƒãŒã„ã‚‚', 'äººå‚', 'ç‰ã­ã', 'ã‚«ãƒ¬ãƒ¼ãƒ«ãƒ¼'],
      'è‚‰ã˜ã‚ƒãŒ': ['ç‰›è‚‰', 'ã˜ã‚ƒãŒã„ã‚‚', 'äººå‚', 'ç‰ã­ã', 'é†¤æ²¹', 'ã¿ã‚Šã‚“'],
      'é‡èœç‚’ã‚': ['ã‚­ãƒ£ãƒ™ãƒ„', 'äººå‚', 'ã‚‚ã‚„ã—', 'ãƒ”ãƒ¼ãƒãƒ³', 'é†¤æ²¹'],
      'é¶': ['é¶è‚‰', 'é†¤æ²¹', 'å¡©', 'èƒ¡æ¤’'],
      'è±š': ['è±šè‚‰', 'é†¤æ²¹', 'å¡©', 'èƒ¡æ¤’'],
      'ç‰›': ['ç‰›è‚‰', 'é†¤æ²¹', 'å¡©', 'èƒ¡æ¤’'],
      'é­š': ['é­š', 'å¡©', 'èƒ¡æ¤’'],
      'ã‚¨ãƒ“': ['ã‚¨ãƒ“', 'å¡©', 'èƒ¡æ¤’'],
      'ã‚¤ã‚«': ['ã‚¤ã‚«', 'é†¤æ²¹', 'ç”Ÿå§œ']
    }

    const ingredients: string[] = []
    
    // ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
    for (const [keyword, items] of Object.entries(ingredientMap)) {
      if (menuName.includes(keyword)) {
        ingredients.push(...items)
        break // æœ€åˆã«ãƒãƒƒãƒã—ãŸã‚‚ã®ã§æ­¢ã‚ã‚‹
      }
    }

    // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆææ–™
    if (ingredients.length === 0) {
      ingredients.push('é†¤æ²¹', 'å¡©', 'èƒ¡æ¤’', 'æ²¹', 'ã”é£¯')
    } else {
      ingredients.push('ã”é£¯') // å®šé£Ÿãªã®ã§ã”é£¯ã¯å¿…é ˆ
    }

    return [...new Set(ingredients)]
  }

  async downloadImage(imageUrl: string, filename: string): Promise<string> {
    if (!imageUrl) return ''
    
    try {
      const response = await fetch(imageUrl)
      const buffer = await response.arrayBuffer()
      
      const imageDir = join(process.cwd(), 'public', 'scraped-images')
      if (!existsSync(imageDir)) {
        mkdirSync(imageDir, { recursive: true })
      }
      
      const imagePath = join(imageDir, `${filename}.jpg`)
      await sharp(Buffer.from(buffer))
        .resize(400, 400, { fit: 'cover' })
        .jpeg({ quality: 80 })
        .toFile(imagePath)
      
      return `/scraped-images/${filename}.jpg`
    } catch (error) {
      console.error('Image download error:', error)
      return ''
    }
  }

  async close() {
    if (this.browser) {
      await this.browser.close()
    }
  }
}

// ä½¿ç”¨ä¾‹
export async function scrapeWithPuppeteer(username: string = 'sense_kabu') {
  const scraper = new TwitterPuppeteerScraper()
  
  try {
    console.log('ğŸ¤– Puppeteerã‚’åˆæœŸåŒ–ä¸­...')
    await scraper.initialize()
    
    console.log(`ğŸ“± @${username} ã®ãƒ„ã‚¤ãƒ¼ãƒˆã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...`)
    const posts = await scraper.scrapeUserPosts(username, 30)
    
    console.log(`ğŸ“¸ ${posts.length}ä»¶ã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...`)
    for (let i = 0; i < posts.length; i++) {
      const post = posts[i]
      if (post.imageUrl) {
        const filename = `menu_${username}_${Date.now()}_${i}`
        post.imageUrl = await scraper.downloadImage(post.imageUrl, filename)
      }
    }
    
    // çµæœã‚’ä¿å­˜
    const outputPath = join(process.cwd(), 'scraped-menus.json')
    writeFileSync(outputPath, JSON.stringify(posts, null, 2), 'utf-8')
    console.log(`ğŸ’¾ çµæœã‚’ä¿å­˜: ${outputPath}`)
    
    return posts
  } finally {
    await scraper.close()
  }
}